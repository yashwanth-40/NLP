{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bags of words:\n",
    "Bag of Words model is used to preprocess the text by converting it into a bag of words, which keeps a count of the total occurrences of most frequently used words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step #1 : We will first preprocess the data, in order to:\n",
    "\n",
    "Convert text to lower case.\n",
    "\n",
    "Remove all non-word characters.\n",
    "\n",
    "Remove all punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np \n",
    "\n",
    "text = my name is harish'\n",
    "dataset = nltk.sent_tokenize(text)\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    dataset[i] = dataset[i].lower()\n",
    "    dataset[i] = re.sub(r'\\W', '' , dataset[i])\n",
    "    dataset[i] = re.sub(r'\\s+', '', dataset[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step #2 : Obtaining most frequent words in our text.\n",
    "\n",
    "We will apply the following steps to generate our model.\n",
    "\n",
    "\n",
    "We declare a dictionary to hold our bag of words.\\\n",
    "Next we tokenize each sentence to words.\\\n",
    "Now for each word in sentence, we check if the word exists in our dictionary.\\\n",
    "If it does, then we increment its count by 1. If it doesnâ€™t, we add it to our dictionary and set its"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2count = {} \n",
    "for data in dataset: \n",
    "    words = nltk.word_tokenize(data) \n",
    "    for word in words: \n",
    "        if word not in word2count.keys(): \n",
    "            word2count[word] = 1\n",
    "        else: \n",
    "            word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq \n",
    "freq_words = heapq.nlargest(100, word2count, key=word2count.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] \n",
    "for data in dataset: \n",
    "    vector = [] \n",
    "    for word in freq_words: \n",
    "        if word in nltk.word_tokenize(data): \n",
    "            vector.append(1) \n",
    "        else: \n",
    "            vector.append(0) \n",
    "    X.append(vector) \n",
    "X = np.asarray(X) "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
